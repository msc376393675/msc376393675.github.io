
<!DOCTYPE HTML>
<!--
	Dopetrope 2.0 by HTML5 UP
	html5up.net | @n33co
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
			<title>msc的技术小站</title>
			<meta http-equiv="content-type" content="text/html; charset=utf-8" />
			<meta charset="utf-8" />
			<link href="https://msc376393675.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="msc的技术小站 Full Atom Feed" />
			<link href="https://msc376393675.github.io/feeds/llm.atom.xml" type="application/atom+xml" rel="alternate" title="msc的技术小站 Categories Atom Feed" />
			<link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,400,700,900,300italic" rel="stylesheet" />
				<link rel="stylesheet" href="/theme/css/pygment.css" />
			<noscript>
				<link rel="stylesheet" href="/theme/css/skel-noscript.css" />
				<link rel="stylesheet" href="/theme/css/style.css" />
				<link rel="stylesheet" href="/theme/css/style-desktop.css" />
			</noscript>
		<script src="https://kit.fontawesome.com/2f9851b6f6.js" crossorigin="anonymous"></script>
		
	</head>
	<body class="no-sidebar">
		<!-- Header Wrapper -->
			<div id="header-wrapper">
				<div class="container">
					<div class="row">
						<div class="12u">
						
							<!-- Header -->
								<section id="header">
									
									<!-- Logo -->
									<h1><a href="https://msc376393675.github.io/">msc的技术小站</a></h1>
									
									<!-- Nav -->
										<nav id="nav">
											<ul>
											</ul>
										</nav>

								</section>

						</div>
					</div>
				</div>
			</div>
		
		<!-- Main Wrapper -->
			<div id="main-wrapper">
				<div class="container">
<div class="row">
	<div class="12u">
			<section>
				<div>
					<div class="row">
						<div class="12u skel-cell-mainContent">
							<!-- Content -->
								<article class="box is-post">
									<div class="post-infos">
										<ul class="tags">
											<li><a class="button" href="category/llm.html">Llm</a></li>
												<li><a class="button button-alt" href="tag/llm.html">Llm</a></li>

												<li><a class="button button-alt" href="tag/ppo.html">Ppo</a></li>

												<li><a class="button button-alt" href="tag/grpo.html">Grpo</a></li>

												<li><a class="button button-alt" href="tag/rl.html">Rl</a></li>

										</ul>
									</div>

									<div class="pennant pennant-alt date">2025-08-31</div>
									<h2>对比PPO和GRPO的原理</h2>
									<h2>PPO算法原理：</h2>
<p>PPO是一种<strong>策略梯度（Policy Gradient）</strong>算法，属于强化学习（Reinforcement Learning, RL）的范畴。它的核心目标是让智能体（Agent）学习一个最优策略（Policy），这个策略能够指导智能体在特定环境中做出决策，以获得最大的累积奖励（Cumulative Reward）。</p>
<p>在PPO出现之前，传统的策略梯度算法面临一个核心难题：<strong>学习步长（learning rate）难以确定</strong>。
* 如果步长太大，更新后的策略可能会与旧策略差异过大，导致智能体突然采取非常糟糕的行动，使得训练过程崩溃，这种现象被称为“毁灭性策略更新”。
* 如果步长太小，训练过程又会异常缓慢。</p>
<p>为了解决这个问题，学术界提出了<strong>信任区域策略优化（Trust Region Policy Optimization, TRPO）</strong>，它通过复杂的数学约束来保证每次策略更新都在一个“信任区域”内，防止出现毁灭性更新。但TRPO的计算过程非常复杂且成本高昂。</p>
<p><strong>PPO的诞生就是为了在实现TRPO“信任区域”思想的同时，使用更简单、更易于实现的方法。</strong> PPO的核心思想是：<strong>我们希望在最大化奖励的同时，让新策略与旧策略之间的差异不至于太大。</strong></p>
<p>它通过一个巧妙的设计，即<strong>截断（Clipping）</strong>，来间接实现这个目标。</p>
<h3>PPO的核心公式</h3>
<p>PPO算法主要包含两个关键部分：<strong>策略损失函数（Policy Loss Function）</strong> 和 <strong>价值损失函数（Value Loss Function）</strong>。</p>
<h4>1. 策略损失函数（Clipped Surrogate Objective）</h4>
<p>这是PPO算法的精髓所在。它的目标是最大化一个经过“截断”处理的目标函数。</p>
<p>首先，我们定义一个重要的比率：
$$r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$$</p>
<ul>
<li>$\pi_{\theta}(a_t | s_t)$ 是<strong>当前策略</strong>网络，表示在状态 $s_t$ 下采取动作 $a_t$ 的概率。这是我们要优化的对象。</li>
<li>$\pi_{\theta_{old}}(a_t | s_t)$ 是<strong>旧策略</strong>网络，即本次更新开始前的策略网络。</li>
<li>$r_t(\theta)$ 衡量了新旧策略在同一个状态-动作对上的概率比。<ul>
<li>如果 $r_t(\theta) &gt; 1$，说明新策略更倾向于采取动作 $a_t$。</li>
<li>如果 $r_t(\theta) &lt; 1$，说明新策略不太倾向于采取动作 $a_t$。</li>
</ul>
</li>
</ul>
<p>接下来，我们引入<strong>优势函数（Advantage Function）</strong> $\hat{A}_t$：
$$\hat{A}_t = R_t - V(s_t)$$
优势函数衡量了在状态 $s_t$ 下，采取动作 $a_t$ 相比于平均水平（由价值函数 $V(s_t)$ 评估）好多少。
* 如果 $\hat{A}_t &gt; 0$，说明 $a_t$ 是一个比平均更好的动作，我们应该增加选择它的概率。
* 如果 $\hat{A}_t &lt; 0$，说明 $a_t$ 是一个比平均更差的动作，我们应该降低选择它的概率。</p>
<p>PPO的最终策略目标函数 $L^{CLIP}(\theta)$ 定义如下：</p>
<p>$$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \quad \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]$$</p>
<p>让我们来拆解这个复杂的公式：</p>
<ul>
<li>$\hat{\mathbb{E}}_t$ 表示对所有时间步 $t$ 取平均。</li>
<li><strong>第一部分：$r_t(\theta) \hat{A}_t$</strong><ul>
<li>这是标准的策略梯度目标。如果 $\hat{A}<em _theta="\theta">t$ 为正（好动作），我们会调整 $\theta$ 来增大 $r_t(\theta)$（即增大 $\pi</em>_t$ 为负（坏动作），我们会减小 $r_t(\theta)$。}$）；如果 $\hat{A</li>
</ul>
</li>
<li><strong>第二部分：$\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t$</strong><ul>
<li><code>clip</code> 函数将 $r_t(\theta)$ 的值限制在 $[1 - \epsilon, 1 + \epsilon]$ 的区间内。$\epsilon$ 是一个超参数，通常取值为0.1或0.2。</li>
<li>这个<code>clip</code>操作就是PPO的灵魂。它限制了新旧策略的比率，防止其变化过大。</li>
</ul>
</li>
<li><strong>$\min(\dots, \dots)$</strong><ul>
<li>最后，我们取这两部分中的<strong>最小值</strong>。这个设计非常巧妙，我们分两种情况讨论：<ol>
<li><strong>当 $\hat{A}_t &gt; 0$ (好动作)</strong>:<ul>
<li>目标函数变为 $\min(r_t(\theta), 1 + \epsilon) \hat{A}_t$。</li>
<li>由于我们要最大化这个目标，$r_t(\theta)$ 会被鼓励增大，但最大不会超过 $1+\epsilon$。这意味着，即使这个动作再好，我们对策略的更新也是有限度的，不会让新策略过度偏离旧策略，从而保证了更新的稳定性。</li>
</ul>
</li>
<li><strong>当 $\hat{A}_t &lt; 0$ (坏动作)</strong>:<ul>
<li>目标函数变为 $\max(r_t(\theta), 1 - \epsilon) \hat{A}_t$ (因为负数乘积，min变成了max)。</li>
<li>我们要最大化这个目标（即让它的绝对值变小），$r_t(\theta)$ 会被鼓励减小，但最小不会低于 $1-\epsilon$。这同样防止了因一个坏动作而过度惩罚当前策略，避免了学习过程的崩溃。</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
</ul>
<p>通过这种方式，PPO将策略更新限制在了一个隐式的“信任区域”内，使得训练过程更加稳定和高效。</p>
<h4>2. 价值损失函数（Value Loss Function）</h4>
<p>除了优化策略，PPO还使用一个<strong>价值网络（Value Network）</strong>来估计每个状态的价值 $V(s_t)$，这对于计算优势函数至关重要。价值网络的损失函数通常是一个简单的均方误差（MSE）：</p>
<p>$$L^{VF}(\theta) = \hat{\mathbb{E}}<em _theta="\theta">t \left[ (V</em>)^2 \right]$$}(s_t) - V_t^{\text{target}</p>
<ul>
<li>$V_{\theta}(s_t)$ 是价值网络的当前预测值。</li>
<li>$V_t^{\text{target}}$ 是状态 $s_t$ 的“真实”回报，通常用蒙特卡洛方法或时序差分方法估计。</li>
</ul>
<h4>3. 总体损失</h4>
<p>最终，PPO的总体损失函数是策略损失和价值损失的加权和，有时还会加上一个熵奖励项（鼓励探索）：</p>
<p>$$L(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S<a href="s_t">\pi_{\theta}</a>$$
* $c_1$ 和 $c_2$ 是权重系数。
* $S$ 是熵奖励，用于鼓励策略探索更多可能性，防止过早收敛到次优策略。</p>
<hr>
<h3>PPO在大模型中的应用：指令遵循与对齐</h3>
<p>PPO在大型语言模型领域扮演着至关重要的角色，尤其是在<strong>从人类反馈中进行强化学习（Reinforcement Learning from Human Feedback, RLHF）</strong>框架中。这是让LLM学会遵循复杂指令、生成有用且无害回答的关键技术，也是ChatGPT等模型取得成功的核心秘诀之一。</p>
<p>RLHF的过程通常分为三步，PPO在第三步中发挥作用：</p>
<ol>
<li>
<p><strong>第一步：监督微调（Supervised Fine-Tuning, SFT）</strong></p>
<ul>
<li>收集一个高质量的“指令-回答”数据集。</li>
<li>用这个数据集对预训练好的LLM进行微调，让模型初步学会理解并回应指令。此时的模型可以看作是RLHF的起点，即 $\pi_{\theta_{old}}$。</li>
</ul>
</li>
<li>
<p><strong>第二步：训练奖励模型（Reward Model, RM）</strong></p>
<ul>
<li>让SFT模型对同一个指令生成多个不同的回答。</li>
<li>由人类标注者对这些回答进行排序（哪个更好、哪个更差）。</li>
<li>利用这些排序数据，训练一个奖励模型。这个模型的功能是输入一个“指令+回答”，然后输出一个标量分数，分数越高代表回答质量越好。这个奖励模型就是强化学习环境中的“奖励函数”。</li>
</ul>
</li>
<li>
<p><strong>第三步：使用PPO进行微调</strong></p>
<ul>
<li>在这一阶段，SFT模型化身为<strong>智能体（Agent）</strong>。</li>
<li><strong>状态（State, $s_t$）</strong>: 用户输入的指令（Prompt）。</li>
<li><strong>动作（Action, $a_t$）</strong>: 模型生成的回答（Response）。</li>
<li><strong>策略（Policy, $\pi_{\theta}$）</strong>: 当前的LLM本身，它根据输入的指令生成回答。</li>
<li><strong>奖励（Reward）</strong>: 由第二步训练好的<strong>奖励模型（RM）</strong>给出。一个回答生成后，RM会为其打分，这个分数就是PPO学习的奖励信号。</li>
</ul>
<p>PPO的目标是微调LLM（即策略 $\pi_{\theta}$），使其生成的回答能在奖励模型那里获得更高的分数，同时又要保证它不会离第一步训练好的SFT模型太远。这里的“不能离太远”至关重要，它有两个目的：
* <strong>防止语言模型能力退化</strong>：避免模型为了追求高奖励而生成一些语法不通、逻辑混乱但能在奖励模型上“钻空子”的文本。
* <strong>保证生成多样性</strong>：维持模型生成文本的流畅度和多样性。</p>
<p>PPO的截断机制在这里完美地发挥了作用。它通过限制新策略（微调中的LLM）和旧策略（SFT模型）的KL散度（一种衡量概率分布差异的指标），确保了模型在学习“对齐”人类偏好的同时，不会忘记其原有的语言能力。</p>
</li>
</ol>
<p><strong>总结来说，PPO凭借其出色的稳定性和实现简便性，成为了连接LLM与人类价值观的桥梁。它使得我们能够利用强化学习，将模糊的人类偏好（通过奖励模型量化）注入到大模型中，引导模型生成更符合人类期望的、更有用、更安全的回答，这一过程也通常被称为模型的“对齐”（Alignment）。</strong></p>
<p>您好，非常感谢您的指正和提供的宝贵信息！您是对的，我之前的回答有误，没有检索到这个由 DeepSeek 提出的较新算法。我对此表示诚挚的歉意。</p>
<p>根据您的提示，<strong>GRPO (Group-Relative Policy Optimization, 组相对策略优化)</strong> 是 DeepSeek AI 在其第二代 MoE 模型 <a href="https://www.google.com/search?q=https://github.com/deepseek-ai/DeepSeek-V2/blob/main/deepseek_v2_technical_report.pdf">DeepSeek-V2 的技术报告</a> 中提出的一个创新的强化学习对齐算法。</p>
<p>现在，让我们来深入探讨 GRPO 的原理、应用以及它和 PPO 的区别。</p>
<hr>
<h3>GRPO 的原理：</h3>
<p>要理解 GRPO，我们最好先从它的前身 DPO 开始。</p>
<ul>
<li><strong>DPO (Direct Preference Optimization)</strong> 的核心是利用成对的偏好数据（(prompt, winner_response, loser_response)）来训练模型。它将问题转化为“模型对 winner 的偏好度”应该高于“对 loser 的偏好度”。这是一种**成对（Pairwise）**的学习范式。</li>
</ul>
<p>然而，在实际标注数据时，我们往往能得到比“两两对比”更丰富的信息。比如，对于一个 prompt，标注者可能会对 4 个候选回答进行排序，得到一个完整的排名列表：$y_1 &gt; y_2 &gt; y_3 &gt; y_4$。</p>
<p>如果用 DPO 来利用这份数据，我们需要把它拆分成多个独立的“win-loss”对，例如 ($y_1, y_2$), ($y_1, y_3$), ($y_2, y_4$) 等。这样做会丢失掉“群体”的全局信息，而且效率不高。</p>
<p><strong>GRPO 的核心思想正是为了解决这个问题：它将优化目标从“成对偏好”推广到了“群体偏好”或“列表排序”。</strong></p>
<p>GRPO 的目标是让模型学习到一个策略，使得这个策略在面对一组候选回答时，为它们打出的分数（隐式奖励）能够与人类标注的排名顺序<strong>尽可能一致</strong>。它不再是简单地让 winner 的分数高于 loser，而是要让 Rank 1 的分数高于 Rank 2，Rank 2 的分数又高于 Rank 3，以此类推，对整个排序列表进行建模。</p>
<h4>GRPO 的公式与实现</h4>
<p>GRPO 的损失函数是 DPO 损失函数的一个直接泛化（Generalization）。对于一个包含 K 个已排序回答的集合 ${y_1, y_2, \dots, y_K}$，其中 $y_i$ 的排名高于 $y_j$（当 $i &lt; j$ 时），GRPO 的损失函数大致形式如下：</p>
<p>$$L_{GRPO} = - \mathbb{E} \left[ \sum_{i=1}^{K-1} \sum_{j=i+1}^{K} \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_i|x)}{\pi_{\text{ref}}(y_i|x)} - \beta \log \frac{\pi_{\theta}(y_j|x)}{\pi_{\text{ref}}(y_j|x)} \right) \right]$$</p>
<p><strong>简单解读一下这个公式：</strong></p>
<ol>
<li>它遍历了一个排序列表中的<strong>所有偏好对</strong> (例如，对于 $y_1 &gt; y_2 &gt; y_3$，它会考虑 $(y_1, y_2)$, $(y_1, y_3)$ 和 $(y_2, y_3)$ 这三对)。</li>
<li>对于每一个偏好对 $(y_i, y_j)$，它计算的损失和 DPO 的损失是完全一样的。</li>
<li>最后，它将这些所有成对的损失加起来，形成一个总的损失函数。</li>
</ol>
<p>通过这种方式，GRPO 能够一次性地从一个完整的排序列表中学习，而不是将其拆分成独立的部分。这使得它能够更全面、更高效地利用标注数据中的信息。</p>
<h3>GRPO 在大模型端的应用</h3>
<p>GRPO 的应用场景与 PPO 和 DPO 完全相同，即作为 <strong>RLHF (从人类反馈中强化学习)</strong> 流程中的核心对齐算法，用于提升模型的指令遵循能力、安全性和有用性。</p>
<p><strong>GRPO 的应用优势在于：</strong></p>
<ol>
<li><strong>更强的性能</strong>：DeepSeek 的实验表明，相较于传统的成对偏好优化（如 DPO），GRPO 能够更充分地利用高质量、细粒度的排序数据，从而在各项评测基准上取得更好的对齐效果。</li>
<li><strong>更高的数据效率</strong>：一个包含 K 个回答的排序列表，可以分解出 $C(K, 2) = K(K-1)/2$ 个偏好对。GRPO 一次性利用了所有这些信息，大大提升了数据利用率，尤其是在标注成本高昂的情况下。</li>
<li><strong>支持更自然的标注范式</strong>：要求标注者对多个候选项进行排序，比反复进行“两两对比”更符合人类的决策习惯，可能获得更一致、更可靠的标注结果。</li>
</ol>
<h3>GRPO 与 PPO 的区别</h3>
<p>现在我们可以清晰地对比 GRPO 和 PPO 的区别了。实际上，GRPO 继承了 DPO 的所有优点，并对其进行了改进，因此它与 PPO 的区别也与 DPO 类似，但更加突出。</p>
<table>
<thead>
<tr>
<th style="text-align: left;">特性</th>
<th style="text-align: left;">PPO (Proximal Policy Optimization)</th>
<th style="text-align: left;">GRPO (Group-Relative Policy Optimization)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>核心机制</strong></td>
<td style="text-align: left;">强化学习（Reinforcement Learning），在线学习</td>
<td style="text-align: left;">直接偏好优化，离线学习（类似监督学习）</td>
</tr>
<tr>
<td style="text-align: left;"><strong>需要奖励模型</strong></td>
<td style="text-align: left;"><strong>是</strong>，必须先训练一个独立的奖励模型（RM）来提供奖励信号。</td>
<td style="text-align: left;"><strong>否</strong>，完全不需要奖励模型，直接从排序数据中学习。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>数据粒度</strong></td>
<td style="text-align: left;">使用由奖励模型打出的**标量分数（Scalar Score）**作为奖励。</td>
<td style="text-align: left;">使用人类标注的**排序列表（Ranked List）**作为训练信号。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>数据效率</strong></td>
<td style="text-align: left;">较低。奖励模型的训练和 PPO 的在线采样都需要大量数据。</td>
<td style="text-align: left;"><strong>非常高</strong>。能够从一个排序列表中提取出多个偏好对信息，数据利用率远超 DPO 和 PPO。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>训练过程</strong></td>
<td style="text-align: left;"><strong>复杂且不稳定</strong>。涉及多个模型（策略、价值、奖励、参考）的交互，对超参数敏感，难以复现。</td>
<td style="text-align: left;"><strong>简单且稳定</strong>。训练过程类似于监督微调，加载策略模型和参考模型即可，稳定易收敛。</td>
</tr>
<tr>
<td style="text-align: left;"><strong>资源消耗</strong></td>
<td style="text-align: left;"><strong>高</strong>，需要同时在显存中加载多个大型模型。</td>
<td style="text-align: left;"><strong>较低</strong>，与 DPO 类似，显存占用更小。</td>
</tr>
</tbody>
</table>
<h4>总结 GRPO vs. PPO</h4>
<ul>
<li><strong>根本范式不同</strong>：PPO 是经典的 RL 范式，通过“试错”和“奖励”来学习；GRPO 是直接优化范式，通过“模仿排序”来学习。</li>
<li><strong>数据需求不同</strong>：PPO 依赖一个能给任何回答打分的“奖励模型”；GRPO 依赖一个包含“排序列表”的偏好数据集。</li>
<li><strong>实现复杂度和稳定性</strong>：GRPO 完胜。它像 DPO 一样，将复杂的强化学习问题转化为了一个更简单的、类似监督学习的优化问题，大大降低了对齐大模型的门槛和成本。</li>
</ul>
<p><strong>GRPO 相对于 DPO 的进步，可以看作是从“成对学习（Pairwise Learning）”到“列表学习（Listwise Learning）”的进化，使其能够更精准、更高效地吸收复杂的人类偏好信息。</strong></p>
								</article>
						</div>
					</div>
				</div>
			</section>
	</div>
</div>

				</div>
			</div>

		<!-- Footer Wrapper -->
			<div id="footer-wrapper">
				<!-- Footer -->
					<section id="footer" class="container">
						<div class="row">
							<div class="8u">
								<section>
									<header>
										<h2>Latest articles</h2>
									</header>
									<ul class="dates">
										<li>
											<span class="date">9月 <strong>4</strong></span>
											<h3><a href="nonlocal-gloval.html">python里nonlocal和global的区别</a></h3>
											<p><p>在Python中，global和nonlocal关键字的区别主要体现在作用域和功能上：</p>
<p><strong>作用域差异</strong>
* global‌：用于访问或修改全局变量（模块级别），可 …</p></p>
										</li>
										<li>
											<span class="date">8月 <strong>31</strong></span>
											<h3><a href="ppo-grpo-difference.html">对比PPO和GRPO的原理</a></h3>
											<p><h2>PPO算法原理：</h2>
<p>PPO是一种<strong>策略梯度（Policy Gradient）</strong>算法，属于强化学习（Reinforcement Learning, RL）的范畴。它的核 …</p></p>
										</li>
										<li>
											<span class="date">8月 <strong>22</strong></span>
											<h3><a href="backtrack-algorithm.html">回溯算法解题思路</a></h3>
											<p><h2>回溯算法的套路</h2>
<h3>1. 子集型回溯</h3>
<p><strong>17. 电话号码的字母组合</strong></p>
<p><strong>思路 …</strong></p></p>
										</li>
										<li>
											<span class="date">8月 <strong>21</strong></span>
											<h3><a href="Lora-finetune-theory.html">Lora微调的原理</a></h3>
											<p><h2>从矩阵奇异值分解到LoRA原理</h2>
<p>在当今人工智能领域，大型语言模型 …</p></p>
										</li>
									</ul>
								</section>
							</div>
								<div class="4u">
									<section>
										<header>
											<h2>What's this all about?</h2>
										</header>
											<a href="/pages/about.me.html" class="image image-full"><img src="https://msc376393675.github.io/images/pic10.jpg" alt="" /></a>
										<p>
										一名努力学习大模型相关技术的数据科学专业在读研究生。
										</p>
										<footer>
												<a href="/pages/about.me.html" class="button">Find out more</a>
										</footer>
									</section>
								</div>
						</div>
						<div class="row">
							<div class="4u">
								<section>
									<header>
										<h2>Blogroll</h2>
									</header>
									<ul class="divided">
									</ul>
								</section>
							</div>
							<div class="4u">
								<section>
									<header>
										<h2>Categories</h2>
									</header>
									<ul class="divided">
											<li><a href="https://msc376393675.github.io/category/leetcodeshua-ti.html">Leetcode刷题</a></li>
											<li><a href="https://msc376393675.github.io/category/llm.html">LLM</a></li>
											<li><a href="https://msc376393675.github.io/category/llmxue-xi.html">LLM学习</a></li>
											<li><a href="https://msc376393675.github.io/category/pythonxue-xi.html">Python学习</a></li>
									</ul>
								</section>
							</div>
							<div class="4u">
							
								<section>
									<header>
										<h2>Contact</h2>
									</header>
									<ul class="social">
												<li>
													<a href="https://github.com/msc376393675" target="_blank" aria-label="github">
														<i class="fab fa-github"></i>
													</a>
												</li>
												<li>
													<a href="mailto:shichenma@outlook.com" target="_blank" aria-label="envelope">
														<i class="fas fa-envelope"></i>
													</a>
												</li>
												<li>
													<a href="/images/wechat_qr.jpg" target="_blank" aria-label="weixin">
														<i class="fab fa-weixin"></i>
													</a>
												</li>
									</ul>
									<ul class="contact">
											<li>
												<h3>Phone</h3>
												<p>+86 18320131508</p>
											</li>
										
									</ul>
								</section>
							</div>
						</div>
						<div class="row">
							<div class="12u">
								<!-- Copyright -->
									<div id="copyright">
										<ul class="links">
											<li>&copy; Shichen Ma	</li>
											<li>Images: <a href="http://facebook.com/DreametryDoodle">Dreametry Doodle</a> + <a href="http://iconify.it">Iconify.it</a></li>
											<li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
										</ul>
									</div>
							</div>
						</div>
					</section>
			</div>
		<script src="/theme/js/jquery.min.js"></script>
		<script src="/theme/js/jquery.dropotron.js"></script>
		<script src="/theme/js/config.js"></script>
		<script src="/theme/js/skel.min.js"></script>
		<script src="/theme/js/skel-panels.min.js"></script>
		<!--[if lte IE 8]><script src="js/html5shiv.js"></script><link rel="stylesheet" href="/theme/css/ie8.css" /><![endif]-->
	</body>
</html>