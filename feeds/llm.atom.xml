<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>msc的技术小站 - LLM</title><link href="https://msc376393675.github.io/" rel="alternate"/><link href="https://msc376393675.github.io/feeds/llm.atom.xml" rel="self"/><id>https://msc376393675.github.io/</id><updated>2025-08-31T21:03:00+08:00</updated><entry><title>对比PPO和GRPO的原理</title><link href="https://msc376393675.github.io/ppo-grpo-difference.html" rel="alternate"/><published>2025-08-31T21:03:00+08:00</published><updated>2025-08-31T21:03:00+08:00</updated><author><name>Shichen Ma</name></author><id>tag:msc376393675.github.io,2025-08-31:/ppo-grpo-difference.html</id><summary type="html">&lt;h2&gt;PPO算法原理：&lt;/h2&gt;
&lt;p&gt;PPO是一种&lt;strong&gt;策略梯度（Policy Gradient）&lt;/strong&gt;算法，属于强化学习（Reinforcement Learning, RL）的范畴。它的核 …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;PPO算法原理：&lt;/h2&gt;
&lt;p&gt;PPO是一种&lt;strong&gt;策略梯度（Policy Gradient）&lt;/strong&gt;算法，属于强化学习（Reinforcement Learning, RL）的范畴。它的核心目标是让智能体（Agent）学习一个最优策略（Policy），这个策略能够指导智能体在特定环境中做出决策，以获得最大的累积奖励（Cumulative Reward）。&lt;/p&gt;
&lt;p&gt;在PPO出现之前，传统的策略梯度算法面临一个核心难题：&lt;strong&gt;学习步长（learning rate）难以确定&lt;/strong&gt;。
* 如果步长太大，更新后的策略可能会与旧策略差异过大，导致智能体突然采取非常糟糕的行动，使得训练过程崩溃，这种现象被称为“毁灭性策略更新”。
* 如果步长太小，训练过程又会异常缓慢。&lt;/p&gt;
&lt;p&gt;为了解决这个问题，学术界提出了&lt;strong&gt;信任区域策略优化（Trust Region Policy Optimization, TRPO）&lt;/strong&gt;，它通过复杂的数学约束来保证每次策略更新都在一个“信任区域”内，防止出现毁灭性更新。但TRPO的计算过程非常复杂且成本高昂。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;PPO的诞生就是为了在实现TRPO“信任区域”思想的同时，使用更简单、更易于实现的方法。&lt;/strong&gt; PPO的核心思想是：&lt;strong&gt;我们希望在最大化奖励的同时，让新策略与旧策略之间的差异不至于太大。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;它通过一个巧妙的设计，即&lt;strong&gt;截断（Clipping）&lt;/strong&gt;，来间接实现这个目标。&lt;/p&gt;
&lt;h3&gt;PPO的核心公式&lt;/h3&gt;
&lt;p&gt;PPO算法主要包含两个关键部分：&lt;strong&gt;策略损失函数（Policy Loss Function）&lt;/strong&gt; 和 &lt;strong&gt;价值损失函数（Value Loss Function）&lt;/strong&gt;。&lt;/p&gt;
&lt;h4&gt;1. 策略损失函数（Clipped Surrogate Objective）&lt;/h4&gt;
&lt;p&gt;这是PPO算法的精髓所在。它的目标是最大化一个经过“截断”处理的目标函数。&lt;/p&gt;
&lt;p&gt;首先，我们定义一个重要的比率：
$$r_t(\theta) = \frac{\pi_{\theta}(a_t | s_t)}{\pi_{\theta_{old}}(a_t | s_t)}$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\pi_{\theta}(a_t | s_t)$ 是&lt;strong&gt;当前策略&lt;/strong&gt;网络，表示在状态 $s_t$ 下采取动作 $a_t$ 的概率。这是我们要优化的对象。&lt;/li&gt;
&lt;li&gt;$\pi_{\theta_{old}}(a_t | s_t)$ 是&lt;strong&gt;旧策略&lt;/strong&gt;网络，即本次更新开始前的策略网络。&lt;/li&gt;
&lt;li&gt;$r_t(\theta)$ 衡量了新旧策略在同一个状态-动作对上的概率比。&lt;ul&gt;
&lt;li&gt;如果 $r_t(\theta) &amp;gt; 1$，说明新策略更倾向于采取动作 $a_t$。&lt;/li&gt;
&lt;li&gt;如果 $r_t(\theta) &amp;lt; 1$，说明新策略不太倾向于采取动作 $a_t$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;接下来，我们引入&lt;strong&gt;优势函数（Advantage Function）&lt;/strong&gt; $\hat{A}_t$：
$$\hat{A}_t = R_t - V(s_t)$$
优势函数衡量了在状态 $s_t$ 下，采取动作 $a_t$ 相比于平均水平（由价值函数 $V(s_t)$ 评估）好多少。
* 如果 $\hat{A}_t &amp;gt; 0$，说明 $a_t$ 是一个比平均更好的动作，我们应该增加选择它的概率。
* 如果 $\hat{A}_t &amp;lt; 0$，说明 $a_t$ 是一个比平均更差的动作，我们应该降低选择它的概率。&lt;/p&gt;
&lt;p&gt;PPO的最终策略目标函数 $L^{CLIP}(\theta)$ 定义如下：&lt;/p&gt;
&lt;p&gt;$$L^{CLIP}(\theta) = \hat{\mathbb{E}}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \quad \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right]$$&lt;/p&gt;
&lt;p&gt;让我们来拆解这个复杂的公式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\hat{\mathbb{E}}_t$ 表示对所有时间步 $t$ 取平均。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第一部分：$r_t(\theta) \hat{A}_t$&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;这是标准的策略梯度目标。如果 $\hat{A}&lt;em _theta="\theta"&gt;t$ 为正（好动作），我们会调整 $\theta$ 来增大 $r_t(\theta)$（即增大 $\pi&lt;/em&gt;_t$ 为负（坏动作），我们会减小 $r_t(\theta)$。}$）；如果 $\hat{A&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;第二部分：$\text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t$&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;&lt;code&gt;clip&lt;/code&gt; 函数将 $r_t(\theta)$ 的值限制在 $[1 - \epsilon, 1 + \epsilon]$ 的区间内。$\epsilon$ 是一个超参数，通常取值为0.1或0.2。&lt;/li&gt;
&lt;li&gt;这个&lt;code&gt;clip&lt;/code&gt;操作就是PPO的灵魂。它限制了新旧策略的比率，防止其变化过大。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;$\min(\dots, \dots)$&lt;/strong&gt;&lt;ul&gt;
&lt;li&gt;最后，我们取这两部分中的&lt;strong&gt;最小值&lt;/strong&gt;。这个设计非常巧妙，我们分两种情况讨论：&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;当 $\hat{A}_t &amp;gt; 0$ (好动作)&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;目标函数变为 $\min(r_t(\theta), 1 + \epsilon) \hat{A}_t$。&lt;/li&gt;
&lt;li&gt;由于我们要最大化这个目标，$r_t(\theta)$ 会被鼓励增大，但最大不会超过 $1+\epsilon$。这意味着，即使这个动作再好，我们对策略的更新也是有限度的，不会让新策略过度偏离旧策略，从而保证了更新的稳定性。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;当 $\hat{A}_t &amp;lt; 0$ (坏动作)&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;目标函数变为 $\max(r_t(\theta), 1 - \epsilon) \hat{A}_t$ (因为负数乘积，min变成了max)。&lt;/li&gt;
&lt;li&gt;我们要最大化这个目标（即让它的绝对值变小），$r_t(\theta)$ 会被鼓励减小，但最小不会低于 $1-\epsilon$。这同样防止了因一个坏动作而过度惩罚当前策略，避免了学习过程的崩溃。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;通过这种方式，PPO将策略更新限制在了一个隐式的“信任区域”内，使得训练过程更加稳定和高效。&lt;/p&gt;
&lt;h4&gt;2. 价值损失函数（Value Loss Function）&lt;/h4&gt;
&lt;p&gt;除了优化策略，PPO还使用一个&lt;strong&gt;价值网络（Value Network）&lt;/strong&gt;来估计每个状态的价值 $V(s_t)$，这对于计算优势函数至关重要。价值网络的损失函数通常是一个简单的均方误差（MSE）：&lt;/p&gt;
&lt;p&gt;$$L^{VF}(\theta) = \hat{\mathbb{E}}&lt;em _theta="\theta"&gt;t \left[ (V&lt;/em&gt;)^2 \right]$$}(s_t) - V_t^{\text{target}&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$V_{\theta}(s_t)$ 是价值网络的当前预测值。&lt;/li&gt;
&lt;li&gt;$V_t^{\text{target}}$ 是状态 $s_t$ 的“真实”回报，通常用蒙特卡洛方法或时序差分方法估计。&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;3. 总体损失&lt;/h4&gt;
&lt;p&gt;最终，PPO的总体损失函数是策略损失和价值损失的加权和，有时还会加上一个熵奖励项（鼓励探索）：&lt;/p&gt;
&lt;p&gt;$$L(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S&lt;a href="s_t"&gt;\pi_{\theta}&lt;/a&gt;$$
* $c_1$ 和 $c_2$ 是权重系数。
* $S$ 是熵奖励，用于鼓励策略探索更多可能性，防止过早收敛到次优策略。&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;PPO在大模型中的应用：指令遵循与对齐&lt;/h3&gt;
&lt;p&gt;PPO在大型语言模型领域扮演着至关重要的角色，尤其是在&lt;strong&gt;从人类反馈中进行强化学习（Reinforcement Learning from Human Feedback, RLHF）&lt;/strong&gt;框架中。这是让LLM学会遵循复杂指令、生成有用且无害回答的关键技术，也是ChatGPT等模型取得成功的核心秘诀之一。&lt;/p&gt;
&lt;p&gt;RLHF的过程通常分为三步，PPO在第三步中发挥作用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第一步：监督微调（Supervised Fine-Tuning, SFT）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;收集一个高质量的“指令-回答”数据集。&lt;/li&gt;
&lt;li&gt;用这个数据集对预训练好的LLM进行微调，让模型初步学会理解并回应指令。此时的模型可以看作是RLHF的起点，即 $\pi_{\theta_{old}}$。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第二步：训练奖励模型（Reward Model, RM）&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;让SFT模型对同一个指令生成多个不同的回答。&lt;/li&gt;
&lt;li&gt;由人类标注者对这些回答进行排序（哪个更好、哪个更差）。&lt;/li&gt;
&lt;li&gt;利用这些排序数据，训练一个奖励模型。这个模型的功能是输入一个“指令+回答”，然后输出一个标量分数，分数越高代表回答质量越好。这个奖励模型就是强化学习环境中的“奖励函数”。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;第三步：使用PPO进行微调&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在这一阶段，SFT模型化身为&lt;strong&gt;智能体（Agent）&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;状态（State, $s_t$）&lt;/strong&gt;: 用户输入的指令（Prompt）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;动作（Action, $a_t$）&lt;/strong&gt;: 模型生成的回答（Response）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略（Policy, $\pi_{\theta}$）&lt;/strong&gt;: 当前的LLM本身，它根据输入的指令生成回答。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励（Reward）&lt;/strong&gt;: 由第二步训练好的&lt;strong&gt;奖励模型（RM）&lt;/strong&gt;给出。一个回答生成后，RM会为其打分，这个分数就是PPO学习的奖励信号。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PPO的目标是微调LLM（即策略 $\pi_{\theta}$），使其生成的回答能在奖励模型那里获得更高的分数，同时又要保证它不会离第一步训练好的SFT模型太远。这里的“不能离太远”至关重要，它有两个目的：
* &lt;strong&gt;防止语言模型能力退化&lt;/strong&gt;：避免模型为了追求高奖励而生成一些语法不通、逻辑混乱但能在奖励模型上“钻空子”的文本。
* &lt;strong&gt;保证生成多样性&lt;/strong&gt;：维持模型生成文本的流畅度和多样性。&lt;/p&gt;
&lt;p&gt;PPO的截断机制在这里完美地发挥了作用。它通过限制新策略（微调中的LLM）和旧策略（SFT模型）的KL散度（一种衡量概率分布差异的指标），确保了模型在学习“对齐”人类偏好的同时，不会忘记其原有的语言能力。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;总结来说，PPO凭借其出色的稳定性和实现简便性，成为了连接LLM与人类价值观的桥梁。它使得我们能够利用强化学习，将模糊的人类偏好（通过奖励模型量化）注入到大模型中，引导模型生成更符合人类期望的、更有用、更安全的回答，这一过程也通常被称为模型的“对齐”（Alignment）。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;您好，非常感谢您的指正和提供的宝贵信息！您是对的，我之前的回答有误，没有检索到这个由 DeepSeek 提出的较新算法。我对此表示诚挚的歉意。&lt;/p&gt;
&lt;p&gt;根据您的提示，&lt;strong&gt;GRPO (Group-Relative Policy Optimization, 组相对策略优化)&lt;/strong&gt; 是 DeepSeek AI 在其第二代 MoE 模型 &lt;a href="https://www.google.com/search?q=https://github.com/deepseek-ai/DeepSeek-V2/blob/main/deepseek_v2_technical_report.pdf"&gt;DeepSeek-V2 的技术报告&lt;/a&gt; 中提出的一个创新的强化学习对齐算法。&lt;/p&gt;
&lt;p&gt;现在，让我们来深入探讨 GRPO 的原理、应用以及它和 PPO 的区别。&lt;/p&gt;
&lt;hr&gt;
&lt;h3&gt;GRPO 的原理：&lt;/h3&gt;
&lt;p&gt;要理解 GRPO，我们最好先从它的前身 DPO 开始。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;DPO (Direct Preference Optimization)&lt;/strong&gt; 的核心是利用成对的偏好数据（(prompt, winner_response, loser_response)）来训练模型。它将问题转化为“模型对 winner 的偏好度”应该高于“对 loser 的偏好度”。这是一种**成对（Pairwise）**的学习范式。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;然而，在实际标注数据时，我们往往能得到比“两两对比”更丰富的信息。比如，对于一个 prompt，标注者可能会对 4 个候选回答进行排序，得到一个完整的排名列表：$y_1 &amp;gt; y_2 &amp;gt; y_3 &amp;gt; y_4$。&lt;/p&gt;
&lt;p&gt;如果用 DPO 来利用这份数据，我们需要把它拆分成多个独立的“win-loss”对，例如 ($y_1, y_2$), ($y_1, y_3$), ($y_2, y_4$) 等。这样做会丢失掉“群体”的全局信息，而且效率不高。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GRPO 的核心思想正是为了解决这个问题：它将优化目标从“成对偏好”推广到了“群体偏好”或“列表排序”。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;GRPO 的目标是让模型学习到一个策略，使得这个策略在面对一组候选回答时，为它们打出的分数（隐式奖励）能够与人类标注的排名顺序&lt;strong&gt;尽可能一致&lt;/strong&gt;。它不再是简单地让 winner 的分数高于 loser，而是要让 Rank 1 的分数高于 Rank 2，Rank 2 的分数又高于 Rank 3，以此类推，对整个排序列表进行建模。&lt;/p&gt;
&lt;h4&gt;GRPO 的公式与实现&lt;/h4&gt;
&lt;p&gt;GRPO 的损失函数是 DPO 损失函数的一个直接泛化（Generalization）。对于一个包含 K 个已排序回答的集合 ${y_1, y_2, \dots, y_K}$，其中 $y_i$ 的排名高于 $y_j$（当 $i &amp;lt; j$ 时），GRPO 的损失函数大致形式如下：&lt;/p&gt;
&lt;p&gt;$$L_{GRPO} = - \mathbb{E} \left[ \sum_{i=1}^{K-1} \sum_{j=i+1}^{K} \log \sigma \left( \beta \log \frac{\pi_{\theta}(y_i|x)}{\pi_{\text{ref}}(y_i|x)} - \beta \log \frac{\pi_{\theta}(y_j|x)}{\pi_{\text{ref}}(y_j|x)} \right) \right]$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;简单解读一下这个公式：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;它遍历了一个排序列表中的&lt;strong&gt;所有偏好对&lt;/strong&gt; (例如，对于 $y_1 &amp;gt; y_2 &amp;gt; y_3$，它会考虑 $(y_1, y_2)$, $(y_1, y_3)$ 和 $(y_2, y_3)$ 这三对)。&lt;/li&gt;
&lt;li&gt;对于每一个偏好对 $(y_i, y_j)$，它计算的损失和 DPO 的损失是完全一样的。&lt;/li&gt;
&lt;li&gt;最后，它将这些所有成对的损失加起来，形成一个总的损失函数。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过这种方式，GRPO 能够一次性地从一个完整的排序列表中学习，而不是将其拆分成独立的部分。这使得它能够更全面、更高效地利用标注数据中的信息。&lt;/p&gt;
&lt;h3&gt;GRPO 在大模型端的应用&lt;/h3&gt;
&lt;p&gt;GRPO 的应用场景与 PPO 和 DPO 完全相同，即作为 &lt;strong&gt;RLHF (从人类反馈中强化学习)&lt;/strong&gt; 流程中的核心对齐算法，用于提升模型的指令遵循能力、安全性和有用性。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;GRPO 的应用优势在于：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;更强的性能&lt;/strong&gt;：DeepSeek 的实验表明，相较于传统的成对偏好优化（如 DPO），GRPO 能够更充分地利用高质量、细粒度的排序数据，从而在各项评测基准上取得更好的对齐效果。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;更高的数据效率&lt;/strong&gt;：一个包含 K 个回答的排序列表，可以分解出 $C(K, 2) = K(K-1)/2$ 个偏好对。GRPO 一次性利用了所有这些信息，大大提升了数据利用率，尤其是在标注成本高昂的情况下。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;支持更自然的标注范式&lt;/strong&gt;：要求标注者对多个候选项进行排序，比反复进行“两两对比”更符合人类的决策习惯，可能获得更一致、更可靠的标注结果。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;GRPO 与 PPO 的区别&lt;/h3&gt;
&lt;p&gt;现在我们可以清晰地对比 GRPO 和 PPO 的区别了。实际上，GRPO 继承了 DPO 的所有优点，并对其进行了改进，因此它与 PPO 的区别也与 DPO 类似，但更加突出。&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style="text-align: left;"&gt;特性&lt;/th&gt;
&lt;th style="text-align: left;"&gt;PPO (Proximal Policy Optimization)&lt;/th&gt;
&lt;th style="text-align: left;"&gt;GRPO (Group-Relative Policy Optimization)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;核心机制&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;强化学习（Reinforcement Learning），在线学习&lt;/td&gt;
&lt;td style="text-align: left;"&gt;直接偏好优化，离线学习（类似监督学习）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;需要奖励模型&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;是&lt;/strong&gt;，必须先训练一个独立的奖励模型（RM）来提供奖励信号。&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;否&lt;/strong&gt;，完全不需要奖励模型，直接从排序数据中学习。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;数据粒度&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;使用由奖励模型打出的**标量分数（Scalar Score）**作为奖励。&lt;/td&gt;
&lt;td style="text-align: left;"&gt;使用人类标注的**排序列表（Ranked List）**作为训练信号。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;数据效率&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;较低。奖励模型的训练和 PPO 的在线采样都需要大量数据。&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;非常高&lt;/strong&gt;。能够从一个排序列表中提取出多个偏好对信息，数据利用率远超 DPO 和 PPO。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;训练过程&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;复杂且不稳定&lt;/strong&gt;。涉及多个模型（策略、价值、奖励、参考）的交互，对超参数敏感，难以复现。&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;简单且稳定&lt;/strong&gt;。训练过程类似于监督微调，加载策略模型和参考模型即可，稳定易收敛。&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;资源消耗&lt;/strong&gt;&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;高&lt;/strong&gt;，需要同时在显存中加载多个大型模型。&lt;/td&gt;
&lt;td style="text-align: left;"&gt;&lt;strong&gt;较低&lt;/strong&gt;，与 DPO 类似，显存占用更小。&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h4&gt;总结 GRPO vs. PPO&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;根本范式不同&lt;/strong&gt;：PPO 是经典的 RL 范式，通过“试错”和“奖励”来学习；GRPO 是直接优化范式，通过“模仿排序”来学习。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;数据需求不同&lt;/strong&gt;：PPO 依赖一个能给任何回答打分的“奖励模型”；GRPO 依赖一个包含“排序列表”的偏好数据集。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;实现复杂度和稳定性&lt;/strong&gt;：GRPO 完胜。它像 DPO 一样，将复杂的强化学习问题转化为了一个更简单的、类似监督学习的优化问题，大大降低了对齐大模型的门槛和成本。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;GRPO 相对于 DPO 的进步，可以看作是从“成对学习（Pairwise Learning）”到“列表学习（Listwise Learning）”的进化，使其能够更精准、更高效地吸收复杂的人类偏好信息。&lt;/strong&gt;&lt;/p&gt;</content><category term="LLM"/><category term="LLM"/><category term="PPO"/><category term="GRPO"/><category term="RL"/></entry></feed>