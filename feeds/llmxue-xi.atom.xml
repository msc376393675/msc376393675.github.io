<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>msc的技术小站 - LLM学习</title><link href="https://msc376393675.github.io/" rel="alternate"/><link href="https://msc376393675.github.io/feeds/llmxue-xi.atom.xml" rel="self"/><id>https://msc376393675.github.io/</id><updated>2025-08-21T20:03:00+08:00</updated><entry><title>Lora微调的原理</title><link href="https://msc376393675.github.io/Lora-finetune-theory.html" rel="alternate"/><published>2025-08-21T20:03:00+08:00</published><updated>2025-08-21T20:03:00+08:00</updated><author><name>Shichen Ma</name></author><id>tag:msc376393675.github.io,2025-08-21:/Lora-finetune-theory.html</id><summary type="html">&lt;h2&gt;从矩阵奇异值分解到LoRA原理&lt;/h2&gt;
&lt;p&gt;在当今人工智能领域，大型语言模型 …&lt;/p&gt;</summary><content type="html">&lt;h2&gt;从矩阵奇异值分解到LoRA原理&lt;/h2&gt;
&lt;p&gt;在当今人工智能领域，大型语言模型（LLM）展现出了惊人的能力，但如何让这些“通才”模型更好地适应特定任务，成为一个“专才”，是学术界和工业界都极为关注的问题。答案就是&lt;strong&gt;微调&lt;/strong&gt;。然而，对动辄拥有数百上千亿参数的庞然大物进行微调，成本极高。&lt;/p&gt;
&lt;p&gt;为了解决这一挑战，一种名为&lt;strong&gt;LoRA（Low-Rank Adaptation，低秩适配）&lt;/strong&gt;的参数高效微调技术应运而生，它通过巧妙的数学原理，实现了“四两拨千斤”的效果，在不牺牲太多性能的前提下，极大降低了微调的成本。要理解LoRA的精髓，我们需要从一个线性代数中的基本概念——&lt;strong&gt;奇异值分解（Singular Value Decomposition, SVD）&lt;/strong&gt;说起。&lt;/p&gt;
&lt;h3&gt;奇异值分解（SVD）：抓住矩阵的“主要矛盾”&lt;/h3&gt;
&lt;p&gt;想象一下，在神经网络中，每一层都包含一个巨大的权重矩阵（$W$），这个矩阵定义了该层如何对输入数据进行线性变换。SVD告诉我们，任何一个矩阵 $W$ 都可以被分解为三个矩阵的乘积：&lt;/p&gt;
&lt;p&gt;$$W = U \Sigma V^T$$&lt;/p&gt;
&lt;p&gt;其中：
* &lt;strong&gt;$U$ 和 $V$&lt;/strong&gt; 是两个&lt;strong&gt;正交矩阵（Orthogonal Matrix）&lt;/strong&gt;。在线性变换的几何意义上，正交矩阵代表着旋转或反射，它不改变数据内部的相对关系和尺寸，只改变其在空间中的朝向。
* &lt;strong&gt;$\Sigma$&lt;/strong&gt; 是一个&lt;strong&gt;对角矩阵（Diagonal Matrix）&lt;/strong&gt;。它的对角线上的元素被称为&lt;strong&gt;奇异值（Singular Values）&lt;/strong&gt;，并且通常从大到小排列。奇异值非常关键，它们衡量了数据在各个方向上被拉伸或缩放的程度。&lt;strong&gt;一个奇异值越大，说明对应的变换方向（特征）在原始矩阵 $W$ 中所占的“信息量”或“重要性”就越高。&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SVD的核心思想在于，它能够揭示出矩阵中最重要的信息。&lt;/strong&gt; 绝大多数情况下，一个大矩阵的很多奇异值都非常小，接近于零。这意味着这些方向上的变换对最终结果的贡献微乎其微。&lt;/p&gt;
&lt;p&gt;这就引出了一个至关重要的应用：&lt;strong&gt;低秩近似（Low-Rank Approximation）&lt;/strong&gt;。我们可以保留 $\Sigma$ 矩阵中最大的前 $k$ 个奇异值，并将其他奇异值置为零，得到一个新的对角矩阵 $\Sigma_k$。然后，通过新的矩阵 $U_k$、$\Sigma_k$ 和 $V_k^T$（$U$ 和 $V$ 的前 $k$ 列/行）相乘，我们就可以得到一个原始矩阵 $W$ 的近似矩阵 $W_k$。&lt;/p&gt;
&lt;p&gt;$$W \approx W_k = U_k \Sigma_k V_k^T$$&lt;/p&gt;
&lt;p&gt;这个新的矩阵 $W_k$ 的“秩”（Rank）为 $k$，远小于原始矩阵的秩。我们用更少的数据（更小的矩阵）保留了原始矩阵绝大部分的信息，抓住了它的“主要矛盾”。&lt;/p&gt;
&lt;h3&gt;全参数更新的代价&lt;/h3&gt;
&lt;p&gt;在介绍LoRA之前，我们先看看传统的&lt;strong&gt;全量微调（Full Fine-tuning）&lt;/strong&gt;。一个预训练好的大模型，其权重矩阵 $W_0$ 包含了从海量数据中学到的通用知识。当我们针对特定任务（如法律文书写作、医疗问答）进行微调时，本质上是在更新这个权重矩阵。&lt;/p&gt;
&lt;p&gt;全量微调的过程是，在新的任务数据上进行训练，通过反向传播计算梯度，对原始权重矩阵 $W_0$ 的每一个参数进行微小的调整，得到一个新的权重矩阵 $W_1 = W_0 + \Delta W$。这里的 $\Delta W$ 就是权重更新量。&lt;/p&gt;
&lt;p&gt;这种方法的&lt;strong&gt;主要问题&lt;/strong&gt;在于：
* &lt;strong&gt;计算成本高昂&lt;/strong&gt;：对于一个百亿参数的模型，存储和计算 $\Delta W$ 需要巨大的显存和计算资源。
* &lt;strong&gt;存储效率低下&lt;/strong&gt;：为每一个新任务都要存储一份完整的、与原始模型同样大小的新模型副本，成本极高。&lt;/p&gt;
&lt;h3&gt;LoRA的破局之道&lt;/h3&gt;
&lt;p&gt;LoRA的提出者们观察到了一个关键现象：在模型适配到新任务的过程中，这个权重更新矩阵 $\Delta W$ 具有“&lt;strong&gt;低内在秩（Low Intrinsic Rank）&lt;/strong&gt;”的特性。通俗地讲，就是这个巨大的更新矩阵，其大部分信息也可以被一个低秩矩阵来近似。&lt;/p&gt;
&lt;p&gt;这与我们前面提到的SVD低秩近似不谋而合。既然 $\Delta W$ 可以被低秩近似，我们何不直接学习一个低秩的表示，而不是去学习整个庞大的 $\Delta W$ 呢？&lt;/p&gt;
&lt;p&gt;LoRA的做法正是如此。它冻结了原始的、巨大的预训练权重矩阵 $W_0$，并在旁边增加了一个“旁路”，这个旁路用来模拟 $\Delta W$。它没有直接去学习一个与 $W_0$ 同样大小的 $\Delta W$，而是学习了两个更小的矩阵 $A$ 和 $B$ 的乘积来近似它：&lt;/p&gt;
&lt;p&gt;$$\Delta W \approx BA$$&lt;/p&gt;
&lt;p&gt;这里的关键在于矩阵 $A$ 和 $B$ 的维度。假设原始权重 $W_0$ 的维度是 $d \times d$，$A$ 的维度是 $d \times r$，$B$ 的维度是 $r \times d$。这里的 $r$ 就是我们设定的&lt;strong&gt;秩（Rank）&lt;/strong&gt;，它远小于 $d$（例如，$r$ 可以是8、16或64，而 $d$ 可能是几千甚至上万）。&lt;/p&gt;
&lt;p&gt;这样一来，在微调过程中：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;原始权重 $W_0$ 保持不变&lt;/strong&gt;，不参与训练。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;我们只训练参数量极小的矩阵 $A$ 和 $B$&lt;/strong&gt;。需要更新的参数数量从 $d \times d$ 锐减到 $d \times r + r \times d = 2dr$。当 $r \ll d$ 时，参数量实现了数量级的下降。&lt;/li&gt;
&lt;li&gt;在进行前向传播时，模型的输出 $y$ 由两部分组成：原始模型的输出和旁路矩阵的输出。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$y = W_0x + \Delta Wx = W_0x + (BA)x$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LoRA与SVD的联系在于，它背后的核心假设与SVD能够实现低秩近似的原理是相通的。&lt;/strong&gt; SVD从理论上证明了任何矩阵都可以通过保留其最重要的奇异值来实现高效的低秩近似。LoRA则在实践中假设模型微调的权重更新也符合这种“信息集中在少数关键方向”的特性，因此可以直接去学习一个低秩的更新表示，从而绕过了计算和存储整个庞大更新矩阵的难题。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LoRA的优势总结：&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;高效训练&lt;/strong&gt;：需要训练的参数量大幅减少，显著降低了对计算资源的需求。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;高效存储和切换&lt;/strong&gt;：对于每个新任务，只需存储和加载小得多的矩阵 $A$ 和 $B$（通常只有几MB），而无需保存整个模型的副本。这使得在多个任务之间切换变得极为方便。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;性能相当&lt;/strong&gt;：大量实验证明，通过精心选择秩 $r$ 和其他超参数，LoRA可以在许多任务上达到与全量微调相媲美甚至更好的性能。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总而言之，从SVD揭示的矩阵内在结构的重要性，到LoRA巧妙地将这一原理应用于大模型微调，我们看到数学理论是如何为解决复杂的工程问题提供深刻洞见的。LoRA通过抓住模型更新的“主要矛盾”，实现了高效、低成本的个性化模型定制，极大地推动了大型语言模型在更广泛领域的应用和发展。&lt;/p&gt;
&lt;h3&gt;LoRA的初始化以及关键参数&lt;/h3&gt;
&lt;p&gt;通常我们会将里&lt;strong&gt;ΔW≈BA&lt;/strong&gt;的&lt;strong&gt;矩阵B&lt;/strong&gt;称为降维矩阵，采用&lt;strong&gt;全的初始化&lt;/strong&gt;，这样在一开始W' = W + ΔW = W, 因为此时 ΔW = 0。&lt;strong&gt;矩阵B&lt;/strong&gt;称为升维矩阵，通常是采用&lt;strong&gt;高斯分布随机初始化&lt;/strong&gt;，这样即可完成初始化步骤。&lt;/p&gt;</content><category term="LLM学习"/><category term="Lora微调"/><category term="LLM"/></entry><entry><title>通过frp实现Ollama内网穿透：从零到一部署指南</title><link href="https://msc376393675.github.io/ollama-frp-deployment.html" rel="alternate"/><published>2025-08-05T10:05:00+08:00</published><updated>2025-08-05T10:05:00+08:00</updated><author><name>Shichen Ma</name></author><id>tag:msc376393675.github.io,2025-08-05:/ollama-frp-deployment.html</id><summary type="html">&lt;p&gt;本文档旨在详细记录如何使用 &lt;code&gt;frp&lt;/code&gt; 工具，将一台部署在内网的 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;本文档旨在详细记录如何使用 &lt;code&gt;frp&lt;/code&gt; 工具，将一台部署在内网的Ollama大模型服务，安全、稳定地暴露到公网上，使得任何能连接互联网的设备都可以访问。&lt;/p&gt;
&lt;h2&gt;准备工作&lt;/h2&gt;
&lt;p&gt;在开始之前，请确保您拥有以下两项基本资源：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Ollama本地机&lt;/strong&gt;：一台已经安装并成功运行Ollama服务的电脑（在本文中，这是一台Linux系统的机器）。这将是我们的 &lt;strong&gt;frpc客户端&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;云服务器&lt;/strong&gt;：一台拥有固定公网IP地址的云服务器（在本文中，这是一台Windows Server 2022系统的京东云主机）。这将是我们的 &lt;strong&gt;frps服务端&lt;/strong&gt;。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;部署步骤&lt;/h2&gt;
&lt;h3&gt;&lt;strong&gt;第1步：在两台机器上准备frp程序&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;frp&lt;/code&gt;的唯一官方下载渠道是其GitHub Releases页面。请为您的两台机器下载对应的正确版本。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;云服务器 (Windows Server)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;下载&lt;/strong&gt;：&lt;code&gt;frp_..._windows_amd64.zip&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;操作&lt;/strong&gt;：在服务器上创建一个永久性文件夹（例如 &lt;code&gt;C:\frp&lt;/code&gt;），并将下载的压缩包解压至此。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ollama本地机 (Linux)&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;下载&lt;/strong&gt;：&lt;code&gt;frp_..._linux_amd64.tar.gz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;操作&lt;/strong&gt;：在本地机上创建一个文件夹（例如 &lt;code&gt;~/frp&lt;/code&gt;），并使用 &lt;code&gt;tar -zxvf ...&lt;/code&gt; 命令将压缩包解压至此。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;&lt;strong&gt;第2步：配置云服务器 (frps服务端)&lt;/strong&gt;&lt;/h3&gt;
&lt;h4&gt;2.1 创建 &lt;code&gt;frps.toml&lt;/code&gt; 配置文件&lt;/h4&gt;
&lt;p&gt;在云服务器的 &lt;code&gt;C:\frp&lt;/code&gt; 目录下，创建一个名为 &lt;code&gt;frps.toml&lt;/code&gt; 的文件，并填入以下内容。这个文件告诉frp服务端要监听哪个端口，以及用于认证的密码。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# C:\frp\frps.toml&lt;/span&gt;
&lt;span class="n"&gt;bindPort&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;7000&lt;/span&gt;
&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;YourSuperSecretToken123&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# 请使用您自己的复杂密码&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;2.2 配置防火墙 (关键步骤)&lt;/h4&gt;
&lt;p&gt;为了让外部请求能够到达&lt;code&gt;frps&lt;/code&gt;服务，需要打通两层防火墙。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;云平台安全组&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;登录您的云服务商（如京东云）的网页控制台。&lt;/li&gt;
&lt;li&gt;找到您服务器实例的“安全组”配置。&lt;/li&gt;
&lt;li&gt;添加入站规则，&lt;strong&gt;允许&lt;/strong&gt;以下两个TCP端口的访问，源IP设置为 &lt;code&gt;0.0.0.0/0&lt;/code&gt;：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;7000&lt;/code&gt; (用于frp客户端和服务端之间的通信)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;11434&lt;/code&gt; (用于外部用户访问您的Ollama穿透服务)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Windows Server系统防火墙&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在服务器上打开“高级安全 Windows Defender 防火墙”。&lt;/li&gt;
&lt;li&gt;创建一条新的“入站规则”，允许TCP协议访问本地端口 &lt;code&gt;7000&lt;/code&gt; 和 &lt;code&gt;11434&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;2.3 设置为Windows服务 (推荐)&lt;/h4&gt;
&lt;p&gt;为了让&lt;code&gt;frps&lt;/code&gt;能在后台长期稳定运行，我们使用&lt;code&gt;nssm&lt;/code&gt;工具将其注册为Windows服务。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;下载&lt;code&gt;nssm&lt;/code&gt;并将其&lt;code&gt;nssm.exe&lt;/code&gt;文件放入&lt;code&gt;C:\frp&lt;/code&gt;目录。&lt;/li&gt;
&lt;li&gt;以管理员身份打开命令提示符，运行 &lt;code&gt;.\nssm.exe install frps&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;在弹出的图形界面中设置：&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Path&lt;/strong&gt;: &lt;code&gt;C:\frp\frps.exe&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Startup directory&lt;/strong&gt;: &lt;code&gt;C:\frp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Arguments&lt;/strong&gt;: &lt;code&gt;-c C:\frp\frps.toml&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;安装并启动服务（通过 &lt;code&gt;nssm start frps&lt;/code&gt; 或 &lt;code&gt;services.msc&lt;/code&gt;）。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;strong&gt;第3步：配置Ollama本地机 (frpc客户端)&lt;/strong&gt;&lt;/h3&gt;
&lt;h4&gt;3.1 创建 &lt;code&gt;frpc.toml&lt;/code&gt; 配置文件&lt;/h4&gt;
&lt;p&gt;在Ollama本地机的 &lt;code&gt;~/frp&lt;/code&gt; 目录下，创建 &lt;code&gt;frpc.toml&lt;/code&gt; 文件，并填入以下内容。这个文件告诉客户端去连接哪个公网服务器，并如何映射本地服务。&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# ~/frp/frpc.toml&lt;/span&gt;
&lt;span class="n"&gt;serverAddr&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;117.72.161.206&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="c1"&gt;# 替换为您的云服务器公网IP&lt;/span&gt;
&lt;span class="n"&gt;serverPort&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;7000&lt;/span&gt;
&lt;span class="n"&gt;auth&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;token&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;YourSuperSecretToken123&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# 必须与frps.toml中的密码一致&lt;/span&gt;

&lt;span class="k"&gt;[[proxies]]&lt;/span&gt;
&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama-service&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;type&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tcp&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;localIP&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;127.0.0.1&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;localPort&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;11434&lt;/span&gt;
&lt;span class="n"&gt;remotePort&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="mi"&gt;11434&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4&gt;3.2 设置为Linux服务 (推荐)&lt;/h4&gt;
&lt;p&gt;同样，为了让&lt;code&gt;frpc&lt;/code&gt;在Linux上后台运行，我们使用&lt;code&gt;systemd&lt;/code&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;赋予执行权限&lt;/strong&gt;:
    &lt;code&gt;bash
    chmod +x ~/frp/frpc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建服务文件&lt;/strong&gt;:
    &lt;code&gt;bash
    sudo nano /etc/systemd/system/frpc.service&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;填入服务配置&lt;/strong&gt; (请务必使用绝对路径):
    ```ini
    [Unit]
    Description=frp client
    After=network.target&lt;/p&gt;
&lt;p&gt;[Service]
Type=simple
User=nobody
Restart=on-failure
RestartSec=5s
ExecStart=/path/to/your/frp/frpc -c /path/to/your/frp/frpc.toml
WorkingDirectory=/path/to/your/frp/&lt;/p&gt;
&lt;p&gt;[Install]
WantedBy=multi-user.target
&lt;code&gt;4.  **启动服务**:&lt;/code&gt;bash
sudo systemctl enable frpc
sudo systemctl start frpc
```&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;&lt;strong&gt;第4步：启动服务并验证&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;确保两端的服务都已启动后，您可以在任何能上网的设备上打开终端，运行以下命令进行最终验证：&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;curl&lt;span class="w"&gt; &lt;/span&gt;http://117.72.161.206:11434
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;如果屏幕上返回 &lt;code&gt;Ollama is running&lt;/code&gt;，则代表您已成功实现内网穿透！&lt;/p&gt;
&lt;h3&gt;&lt;strong&gt;第5步：在代码中调用&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;最后，将您应用程序中调用Ollama的地址，更新为新的公网地址即可。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;示例 (&lt;code&gt;llm_wrapper.py&lt;/code&gt;)&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;OLLAMA_BASE_URL&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;environ&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;OLLAMA_BASE_URL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;http://117.72.161.206:11434&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;hr&gt;</content><category term="LLM学习"/><category term="Ollama"/><category term="frp"/><category term="内网穿透"/><category term="服务器"/></entry><entry><title>Llama Factory 微调详细步骤记录</title><link href="https://msc376393675.github.io/llama-factory-finetune-steps.html" rel="alternate"/><published>2025-07-21T10:00:00+08:00</published><updated>2025-07-21T10:00:00+08:00</updated><author><name>Shichen Ma</name></author><id>tag:msc376393675.github.io,2025-07-21:/llama-factory-finetune-steps.html</id><summary type="html">&lt;p&gt;本文档旨在提供一个清晰、可重复的端到端操作流程，涵盖从 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;本文档旨在提供一个清晰、可重复的端到端操作流程，涵盖从 &lt;strong&gt;安装 Llama Factory&lt;/strong&gt;、&lt;strong&gt;执行模型微调&lt;/strong&gt;，到最终将微调产物&lt;strong&gt;部署到 Ollama 本地服务&lt;/strong&gt;的全过程。&lt;/p&gt;
&lt;h2&gt;章节一：Llama Factory 的安装与启动&lt;/h2&gt;
&lt;p&gt;本章节指导您完成 Llama Factory 的环境准备和安装。&lt;/p&gt;
&lt;h3&gt;1. 环境准备 (Environment Preparation)&lt;/h3&gt;
&lt;p&gt;在开始之前，请确保您的系统满足以下条件：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;操作系统&lt;/strong&gt;: Windows 10/11。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;硬件&lt;/strong&gt;: 一块支持 CUDA 的 NVIDIA 显卡 (建议 8GB VRAM 以上)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;软件&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Git&lt;/strong&gt;: 用于克隆项目仓库。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Conda&lt;/strong&gt;: 用于管理 Python 环境，强烈推荐。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Python&lt;/strong&gt;: 版本需为 &lt;code&gt;3.10&lt;/code&gt; 或 &lt;code&gt;3.11&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUDA Toolkit&lt;/strong&gt;: 与您的显卡驱动兼容的版本 (例如 &lt;code&gt;11.8&lt;/code&gt; 或 &lt;code&gt;12.1&lt;/code&gt;)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Visual Studio&lt;/strong&gt;: 已安装，并包含 "使用 C++ 的桌面开发" 工作负载 (这是编译 &lt;code&gt;llama.cpp&lt;/code&gt; 所必需的)。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;2. 安装 Llama Factory&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;打开终端&lt;/strong&gt; (推荐使用 Conda 自带的 Anaconda Prompt)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建并激活 Conda 环境&lt;/strong&gt;：
    &lt;code&gt;bash
    conda create --name sft python=3.10 -y
    conda activate sft&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;安装 PyTorch&lt;/strong&gt; (请根据您的 CUDA 版本从 &lt;a href="https://pytorch.org/get-started/locally/"&gt;PyTorch 官网&lt;/a&gt; 获取对应的命令)：
    &lt;code&gt;bash
    # 示例 (CUDA 12.1)
    pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;克隆 Llama Factory 仓库&lt;/strong&gt;：
    &lt;code&gt;bash
    git clone https://github.com/hiyouga/LLaMA-Factory.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;进入项目目录并安装依赖&lt;/strong&gt;：
    &lt;code&gt;bash
    cd LLaMA-Factory
    pip install -e .[metrics]&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;3. 启动 Web UI&lt;/h3&gt;
&lt;p&gt;Llama Factory 提供了便捷的图形化界面来简化微调操作。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;在已激活 &lt;code&gt;sft&lt;/code&gt; 环境的终端中，执行以下命令：
    &lt;code&gt;bash
    llamafactory-cli webui&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;启动成功后，终端会显示一个本地网址 (通常是 &lt;code&gt;http://127.0.0.1:7860&lt;/code&gt;)。在浏览器中打开此地址，即可看到 Llama Factory 的操作界面。&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;章节二：使用 Llama Factory 进行模型微调&lt;/h2&gt;
&lt;p&gt;本章节简述在 Web UI 中完成一次典型的 LoRA 微调任务的核心步骤。&lt;/p&gt;
&lt;p&gt;微调完成后，您将得到一个 LoRA 适配器文件夹，&lt;strong&gt;这正是我们后续部署操作的起点&lt;/strong&gt;。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;选择模型 (Model)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;模型名称&lt;/code&gt; 下拉框中，选择或输入您要微调的基础模型，例如 &lt;code&gt;Qwen/Qwen2-0.5B&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;Llama Factory 会自动从 Hugging Face 下载模型。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;选择微调方法 (Train -&amp;gt; Method)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;微调方法&lt;/code&gt; 下拉框中，选择 &lt;code&gt;lora&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;选择数据集 (Train -&amp;gt; Dataset)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;数据集&lt;/code&gt; 下拉框中，选择您用于训练的数据集。您可以使用内置的数据集，也可以上传自己的数据。&lt;/li&gt;
&lt;li&gt;对于意图识别等任务，通常使用自定义的 &lt;code&gt;json&lt;/code&gt; 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;设置训练参数 (Train -&amp;gt; Arguments)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;输出目录&lt;/strong&gt;: 这里是微调产物（LoRA适配器）的保存位置。默认在 &lt;code&gt;LLaMA-Factory/saves/&lt;/code&gt; 目录下。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;学习率&lt;/strong&gt;: 例如 &lt;code&gt;1e-4&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;训练轮数&lt;/strong&gt;: 例如 &lt;code&gt;3.0&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LoRA 秩 (LoRA rank)&lt;/strong&gt;: 通常设为 &lt;code&gt;8&lt;/code&gt; 或 &lt;code&gt;16&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;最大样本长度&lt;/strong&gt;: 根据您的数据和显存进行调整，例如 &lt;code&gt;1024&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;开始训练 (Run -&amp;gt; Start)&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;点击 &lt;code&gt;开始&lt;/code&gt; 按钮启动微调过程。您可以在终端和UI的日志区域观察训练进度。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;找到训练产物&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;训练完成后，进入您设置的&lt;strong&gt;输出目录&lt;/strong&gt;，例如 &lt;code&gt;D:\GitProjects\LLaMA-Factory\saves\Qwen2-0.5B\lora\train_...&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;这个文件夹就是我们需要的 &lt;strong&gt;&lt;code&gt;[你的LoRA适配器路径]&lt;/code&gt;&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;（以下是您提供的原始内容，已重构为后续步骤）&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;章节三：合并 LoRA 适配器&lt;/h2&gt;
&lt;p&gt;此步骤将 LoRA 权重合并到基础模型中，生成一个完整的、可独立运行的 Hugging Face 格式模型。&lt;/p&gt;
&lt;h3&gt;1. 准备工作&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;项目目录&lt;/strong&gt;: 按照以下结构手动创建文件夹，用于存放后续所有产物。
    &lt;code&gt;D:/AI_Models/
    └── my-intent-model/
        ├── 1_merged_hf/
        ├── 2_gguf_conversion/
        └── 3_ollama_deploy/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;占位符&lt;/strong&gt;: 在后续命令中，请将以下占位符替换为您的实际信息：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;[基础模型名称]&lt;/code&gt;: &lt;code&gt;Qwen/Qwen2-0.5B&lt;/code&gt; (与微调时使用的模型一致)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[你的LoRA适配器路径]&lt;/code&gt;: &lt;code&gt;D:\GitProjects\LLaMA-Factory\saves\Qwen2-0.5B\lora\train_...&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[你的项目总目录]&lt;/code&gt;: &lt;code&gt;D:\AI_Models\my-intent-model&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[你的llama.cpp目录]&lt;/code&gt;: &lt;code&gt;D:\GitProjects\llama.cpp&lt;/code&gt; (假设您已克隆并编译好)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;[你的Ollama模型名称]&lt;/code&gt;: &lt;code&gt;my-intent-model&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 执行合并&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;打开终端&lt;/strong&gt; (cmd 或 PowerShell)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;激活 Conda 环境&lt;/strong&gt;：
    &lt;code&gt;bash
    conda activate sft&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;执行合并命令&lt;/strong&gt; (Windows &lt;code&gt;cmd&lt;/code&gt; 中使用 &lt;code&gt;^&lt;/code&gt; 符号换行，方便阅读)：
    &lt;code&gt;bash
    llamafactory-cli export ^
        --model_name_or_path [基础模型名称] ^
        --adapter_name_or_path "[你的LoRA适配器路径]" ^
        --template qwen ^
        --export_dir "[你的项目总目录]\1_merged_hf"&lt;/code&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;产物&lt;/strong&gt;：此命令成功后，&lt;code&gt;1_merged_hf&lt;/code&gt; 文件夹内会包含合并好的模型文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;章节四：转换为 GGUF 格式&lt;/h2&gt;
&lt;p&gt;此步骤将 Hugging Face 格式的模型转换为 &lt;code&gt;llama.cpp&lt;/code&gt; 和 Ollama 使用的 GGUF 格式，并进行量化以减小体积。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;打开“Developer Command Prompt for VS”&lt;/strong&gt; (VS 开发者命令提示符)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;激活 Conda 环境&lt;/strong&gt;：
    &lt;code&gt;bash
    conda activate sft&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;4.1 转换为 F16 GGUF (未量化)&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;python&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[你的llama.cpp目录]\convert_hf_to_gguf.py&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[你的项目总目录]\1_merged_hf&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;--outfile&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[你的项目总目录]\2_gguf_conversion\model-f16.gguf&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;--outtype&lt;span class="w"&gt; &lt;/span&gt;f16
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;产物&lt;/strong&gt;：生成一个较大的 &lt;code&gt;model-f16.gguf&lt;/code&gt; 文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4.2 量化为 Q4_K_M GGUF (最终模型)&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="s2"&gt;&amp;quot;[你的llama.cpp目录]\build\bin\Release\llama-quantize.exe&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[你的项目总目录]\2_gguf_conversion\model-f16.gguf&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;[你的项目总目录]\2_gguf_conversion\model-q4_k_m.gguf&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;q4_k_m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;产物&lt;/strong&gt;：生成最终的模型文件 &lt;code&gt;model-q4_k_m.gguf&lt;/code&gt;。&lt;strong&gt;这是部署到 Ollama 所需的核心文件。&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;章节五：加载到 Ollama&lt;/h2&gt;
&lt;h3&gt;1. 创建 Modelfile&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;在 &lt;code&gt;[你的项目总目录]\3_ollama_deploy\&lt;/code&gt; 文件夹内，创建一个名为 &lt;code&gt;Modelfile&lt;/code&gt; 的文件（&lt;strong&gt;没有文件后缀名&lt;/strong&gt;）。&lt;/li&gt;
&lt;li&gt;文件内容应包含以下一行，使用相对路径指向最终的 GGUF 文件：
    &lt;code&gt;FROM ../2_gguf_conversion/model-q4_k_m.gguf&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2. 创建并运行 Ollama 模型&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;打开一个&lt;strong&gt;新的、普通的&lt;/strong&gt;终端窗口（cmd 或 PowerShell）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;必须&lt;/strong&gt;先切换到 &lt;code&gt;3_ollama_deploy&lt;/code&gt; 目录，这样 &lt;code&gt;Modelfile&lt;/code&gt; 中的相对路径才能生效。
    &lt;code&gt;bash
    cd /d "[你的项目总目录]\3_ollama_deploy"&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;创建模型&lt;/strong&gt;：
    &lt;code&gt;bash
    ollama create [你的Ollama模型名称] -f ./Modelfile&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;运行模型&lt;/strong&gt;：
    &lt;code&gt;bash
    ollama run [你的Ollama模型名称]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;章节六：清理冗余文件&lt;/h2&gt;
&lt;p&gt;在您确认 Ollama 模型可以成功运行后，可以删除以下中间文件以释放大量磁盘空间。&lt;/p&gt;
&lt;h3&gt;可以安全删除的产物&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;[你的项目总目录]\1_merged_hf\&lt;/code&gt;&lt;/strong&gt; (文件夹)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;说明&lt;/strong&gt;：合并后的全精度 Hugging Face 模型，体积较大，仅用于生成 GGUF。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;[你的项目总目录]\2_gguf_conversion\model-f16.gguf&lt;/code&gt;&lt;/strong&gt; (文件)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;说明&lt;/strong&gt;：中间步骤生成的全精度 GGUF 文件，体积非常大，仅用于生成最终的量化版 GGUF。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;建议保留的产物&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;[你的项目总目录]\2_gguf_conversion\model-q4_k_m.gguf&lt;/code&gt;&lt;/strong&gt; (文件)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;说明&lt;/strong&gt;：您所有工作的最终产物，是模型的“母版”。强烈建议&lt;strong&gt;备份并保留&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;[你的项目总目录]\3_ollama_deploy\&lt;/code&gt;&lt;/strong&gt; (文件夹)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;说明&lt;/strong&gt;：包含了 &lt;code&gt;Modelfile&lt;/code&gt;，是您模型的“配方”，建议和 GGUF 文件一起保留。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;code&gt;[你的llama.cpp目录]\build\&lt;/code&gt;&lt;/strong&gt; (文件夹)&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;说明&lt;/strong&gt;：包含了编译好的 &lt;code&gt;llama-quantize.exe&lt;/code&gt; 等工具，保留它可以让您在处理下一个模型时跳过编译步骤。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="LLM学习"/><category term="Llama Factory"/><category term="LLM"/><category term="微调"/></entry></feed>